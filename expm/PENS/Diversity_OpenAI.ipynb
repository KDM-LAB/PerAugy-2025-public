{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load KBERT tokenizer and model (assuming KBERT is in Hugging Face)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-bert-wwm-ext\")\n",
    "model = BertModel.from_pretrained(\"hfl/chinese-bert-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_kbert_embedding(sentence):\n",
    "    \"\"\"Get KBERT embedding for a given sentence.\"\"\"\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the [CLS] token's embedding (for sentence-level representation)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    return cls_embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def kbert_distance(sent1, sent2):\n",
    "    \"\"\"Calculate MSE-based distance between two sentences using KBERT.\"\"\"\n",
    "    emb1 = get_kbert_embedding(sent1)\n",
    "    emb2 = get_kbert_embedding(sent2)\n",
    "    \n",
    "    # Ensure the embeddings have the same shape\n",
    "    if emb1.shape != emb2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape to calculate MSE distance\")\n",
    "    \n",
    "    # Compute Mean Squared Error (MSE) between embeddings\n",
    "    mse_distance = np.mean((emb1 - emb2) ** 2)\n",
    "    \n",
    "    return mse_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def kbert_based_divergence(Dt1, Dt2, Ut1, Ut2):\n",
    "    \"\"\"Calculate divergence using KBERT embeddings.\"\"\"\n",
    "    # Calculate KBERT-based distances between user summaries (Ut1, Ut2) and document texts (Dt1, Dt2)\n",
    "    document_dist1 = kbert_distance(Dt1, Ut1)\n",
    "    document_dist2 = kbert_distance(Dt2, Ut2)\n",
    "    doc_dist_t1_t2 = kbert_distance(Dt1, Dt2)\n",
    "    user_dist_t1_t2 = kbert_distance(Ut1, Ut2)\n",
    "\n",
    "    # Compute divergence based on the distances\n",
    "    if user_dist_t1_t2 != 0:\n",
    "        divergence_value = (doc_dist_t1_t2 / user_dist_t1_t2) * 0.5 * (document_dist2 / document_dist1)\n",
    "    else:\n",
    "        divergence_value = 0  # Handle the case where distance is zero\n",
    "    \n",
    "    return divergence_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Processing user trajectories\"):\n",
    "    actions = row['Action'].split(',')\n",
    "    docs = row['Docs'].split(',')\n",
    "    cleaned_actions = [act.strip(\" '\") for act in actions]\n",
    "    docs = [d.strip(\" '\") for d in docs]\n",
    "\n",
    "    for i, (action, doc) in enumerate(zip(cleaned_actions, docs)):\n",
    "        if action == 'gen_summ':\n",
    "            summ_id = docs[i+1]\n",
    "            summ_row = summ_df[summ_df['SummID'] == summ_id]\n",
    "\n",
    "            if summ_row.empty:\n",
    "                continue\n",
    "\n",
    "            Ut1 = summ_row['Summary'].values[0]\n",
    "            Dt1 = summ_row['NewsID'].values[0]\n",
    "\n",
    "            for j in range(i + 1, len(cleaned_actions)):\n",
    "                next_action = cleaned_actions[j]\n",
    "                if next_action == 'gen_summ':\n",
    "                    next_summ_id = docs[j+1]\n",
    "                    next_summ_row = summ_df[summ_df['SummID'] == next_summ_id]\n",
    "\n",
    "                    if next_summ_row.empty:\n",
    "                        break\n",
    "\n",
    "                    Ut2 = next_summ_row['Summary'].values[0]\n",
    "                    Dt2 = next_summ_row['NewsID'].values[0]\n",
    "\n",
    "                    # Calculate KBERT-based document divergence\n",
    "                    document_divergence = kbert_based_divergence(Dt1, Dt2, Ut1, Ut2)\n",
    "                    overall_divergence += document_divergence\n",
    "\n",
    "                    # Update Ut1 and Dt1 for next iteration\n",
    "                    Ut1, Dt1 = Ut2, Dt2\n",
    "\n",
    "overall_divergence = Overall_divergence/len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
