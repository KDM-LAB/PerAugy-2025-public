{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Yale-LILY/brio-xsum-cased\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Yale-LILY/brio-xsum-cased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/teamspace/studios/this_studio/NLP_project/openai_reddit_final.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83797 entries, 0 to 83796\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    83797 non-null  int64 \n",
      " 1   user_id       83797 non-null  object\n",
      " 2   doc_id        83797 non-null  object\n",
      " 3   user_profile  83797 non-null  object\n",
      " 4   post/article  83797 non-null  object\n",
      " 5   summary_text  83797 non-null  object\n",
      " 6   confidence    83797 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PegasusTokenizerFast(name_or_path='Yale-LILY/brio-xsum-cased', vocab_size=96103, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask_2>', 'additional_special_tokens': ['<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"<mask_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t3: AddedToken(\"<mask_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t4: AddedToken(\"<unk_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t5: AddedToken(\"<unk_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t6: AddedToken(\"<unk_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t7: AddedToken(\"<unk_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t8: AddedToken(\"<unk_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t9: AddedToken(\"<unk_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t10: AddedToken(\"<unk_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t11: AddedToken(\"<unk_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t12: AddedToken(\"<unk_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t13: AddedToken(\"<unk_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t14: AddedToken(\"<unk_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t15: AddedToken(\"<unk_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t16: AddedToken(\"<unk_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t17: AddedToken(\"<unk_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t18: AddedToken(\"<unk_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t19: AddedToken(\"<unk_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t20: AddedToken(\"<unk_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t21: AddedToken(\"<unk_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t22: AddedToken(\"<unk_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t23: AddedToken(\"<unk_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t24: AddedToken(\"<unk_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t25: AddedToken(\"<unk_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t26: AddedToken(\"<unk_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t27: AddedToken(\"<unk_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t28: AddedToken(\"<unk_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t29: AddedToken(\"<unk_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t30: AddedToken(\"<unk_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t31: AddedToken(\"<unk_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t32: AddedToken(\"<unk_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t33: AddedToken(\"<unk_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t34: AddedToken(\"<unk_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t35: AddedToken(\"<unk_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t36: AddedToken(\"<unk_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t37: AddedToken(\"<unk_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t38: AddedToken(\"<unk_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t39: AddedToken(\"<unk_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t40: AddedToken(\"<unk_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t41: AddedToken(\"<unk_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t42: AddedToken(\"<unk_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t43: AddedToken(\"<unk_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t44: AddedToken(\"<unk_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t45: AddedToken(\"<unk_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t46: AddedToken(\"<unk_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t47: AddedToken(\"<unk_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t48: AddedToken(\"<unk_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t49: AddedToken(\"<unk_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t50: AddedToken(\"<unk_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t51: AddedToken(\"<unk_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t52: AddedToken(\"<unk_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t53: AddedToken(\"<unk_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t54: AddedToken(\"<unk_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t55: AddedToken(\"<unk_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t56: AddedToken(\"<unk_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t57: AddedToken(\"<unk_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t58: AddedToken(\"<unk_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t59: AddedToken(\"<unk_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t60: AddedToken(\"<unk_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t61: AddedToken(\"<unk_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t62: AddedToken(\"<unk_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t63: AddedToken(\"<unk_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t64: AddedToken(\"<unk_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t65: AddedToken(\"<unk_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t66: AddedToken(\"<unk_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t67: AddedToken(\"<unk_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t68: AddedToken(\"<unk_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t69: AddedToken(\"<unk_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t70: AddedToken(\"<unk_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t71: AddedToken(\"<unk_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t72: AddedToken(\"<unk_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t73: AddedToken(\"<unk_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t74: AddedToken(\"<unk_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t75: AddedToken(\"<unk_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t76: AddedToken(\"<unk_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t77: AddedToken(\"<unk_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t78: AddedToken(\"<unk_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t79: AddedToken(\"<unk_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t80: AddedToken(\"<unk_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t81: AddedToken(\"<unk_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t82: AddedToken(\"<unk_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t83: AddedToken(\"<unk_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t84: AddedToken(\"<unk_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t85: AddedToken(\"<unk_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t86: AddedToken(\"<unk_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t87: AddedToken(\"<unk_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t88: AddedToken(\"<unk_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t89: AddedToken(\"<unk_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t90: AddedToken(\"<unk_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t91: AddedToken(\"<unk_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t92: AddedToken(\"<unk_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t93: AddedToken(\"<unk_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t94: AddedToken(\"<unk_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t95: AddedToken(\"<unk_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t96: AddedToken(\"<unk_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t97: AddedToken(\"<unk_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t98: AddedToken(\"<unk_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t99: AddedToken(\"<unk_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t100: AddedToken(\"<unk_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t101: AddedToken(\"<unk_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t102: AddedToken(\"<unk_100>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t103: AddedToken(\"<unk_101>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t104: AddedToken(\"<unk_102>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t105: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " },\n",
       " PegasusForConditionalGeneration(\n",
       "   (model): PegasusModel(\n",
       "     (shared): Embedding(96103, 1024, padding_idx=0)\n",
       "     (encoder): PegasusEncoder(\n",
       "       (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "       (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-15): 16 x PegasusEncoderLayer(\n",
       "           (self_attn): PegasusAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): ReLU()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): PegasusDecoder(\n",
       "       (embed_tokens): Embedding(96103, 1024, padding_idx=0)\n",
       "       (embed_positions): PegasusSinusoidalPositionalEmbedding(512, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-15): 16 x PegasusDecoderLayer(\n",
       "           (self_attn): PegasusAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): ReLU()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): PegasusAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=1024, out_features=96103, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['doc_id', 'post/article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83797 entries, 0 to 83796\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   doc_id        83797 non-null  object\n",
      " 1   post/article  83797 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = {}\n",
    "\n",
    "for idx , row in data.iterrows():\n",
    "    doc_id = row['doc_id']\n",
    "    text = row['post/article']\n",
    "\n",
    "    if doc_id not in mapp:\n",
    "        mapp[doc_id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list(mapp.keys())\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_texts = list(mapp.values())\n",
    "len(unique_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "def translate_batch(texts):\n",
    "    batch = tokenizer(texts, truncation=True, padding='longest', return_tensors=\"pt\").to(device)\n",
    "    translated = model.generate(**batch)\n",
    "    tgt_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return tgt_texts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_texts = unique_texts\n",
    "len(src_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(src_texts), batch_size):\n",
    "    batch_texts = src_texts[i:i + batch_size]\n",
    "    translated_texts = translate_batch(batch_texts)\n",
    "    results.extend(translated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_summaries = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = dict(zip(ids, gen_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6218"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83797 entries, 0 to 83796\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    83797 non-null  int64 \n",
      " 1   user_id       83797 non-null  object\n",
      " 2   doc_id        83797 non-null  object\n",
      " 3   user_profile  83797 non-null  object\n",
      " 4   post/article  83797 non-null  object\n",
      " 5   summary_text  83797 non-null  object\n",
      " 6   confidence    83797 non-null  int64 \n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brio_model_summary'] = df['doc_id'].map(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 83797 entries, 0 to 83796\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          83797 non-null  int64 \n",
      " 1   user_id             83797 non-null  object\n",
      " 2   doc_id              83797 non-null  object\n",
      " 3   user_profile        83797 non-null  object\n",
      " 4   post/article        83797 non-null  object\n",
      " 5   summary_text        83797 non-null  object\n",
      " 6   confidence          83797 non-null  int64 \n",
      " 7   brio_model_summary  83797 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('openai_reddit_final_brio.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
