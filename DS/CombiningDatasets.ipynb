{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62d5a1ca-47f5-4899-a665-e3dc807c3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script running from: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA\n",
      "\n",
      "Using paths:\n",
      "Input folder: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA\n",
      "Output file: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA/combined_sample.csv\n",
      "Looking for CSV files in: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA\n",
      "Current working directory: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA\n",
      "\n",
      "All files in directory:\n",
      "- Combined_Sampled.ipynb\n",
      "- synthetic-original_openai_300_10.csv\n",
      "- synthetic-original_openai_300_20.csv\n",
      "- synthetic-original_openai_300_30.csv\n",
      "- synthetic-original_openai_300_40.csv\n",
      "- synthetic-original_openai_300_50.csv\n",
      "- synthetic-original_openai_50_100.csv\n",
      "- synthetic-original_openai_50_150.csv\n",
      "- synthetic-original_openai_50_200.csv\n",
      "- synthetic-original_openai_50_350.csv\n",
      "- synthetic-original_openai_50_500.csv\n",
      "\n",
      "Found 10 CSV files\n",
      "\n",
      "CSV files to be processed:\n",
      "- synthetic-original_openai_300_10.csv\n",
      "- synthetic-original_openai_300_20.csv\n",
      "- synthetic-original_openai_300_30.csv\n",
      "- synthetic-original_openai_300_40.csv\n",
      "- synthetic-original_openai_300_50.csv\n",
      "- synthetic-original_openai_50_100.csv\n",
      "- synthetic-original_openai_50_150.csv\n",
      "- synthetic-original_openai_50_200.csv\n",
      "- synthetic-original_openai_50_350.csv\n",
      "- synthetic-original_openai_50_500.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  20%|██        | 2/10 [00:00<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing synthetic-original_openai_300_10.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_300_20.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_300_30.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  40%|████      | 4/10 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing synthetic-original_openai_300_40.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_300_50.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_50_100.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_50_150.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 10/10 [00:00<00:00, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing synthetic-original_openai_50_200.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_50_350.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n",
      "\n",
      "Processing synthetic-original_openai_50_500.csv\n",
      "Original size: 1280 rows\n",
      "Sampled size: 128 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "Total rows in combined dataset: 1280\n",
      "Output saved to: /teamspace/studios/this_studio/PENS-Personalized-News-Headline-Generation/ZDATA/combined_sample.csv\n",
      "\n",
      "Samples per source file:\n",
      "source_file\n",
      "synthetic-original_openai_300_10.csv    128\n",
      "synthetic-original_openai_300_20.csv    128\n",
      "synthetic-original_openai_300_30.csv    128\n",
      "synthetic-original_openai_300_40.csv    128\n",
      "synthetic-original_openai_300_50.csv    128\n",
      "synthetic-original_openai_50_100.csv    128\n",
      "synthetic-original_openai_50_150.csv    128\n",
      "synthetic-original_openai_50_200.csv    128\n",
      "synthetic-original_openai_50_350.csv    128\n",
      "synthetic-original_openai_50_500.csv    128\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def sample_and_combine_csvs(input_folder, output_file, sample_size=40000):\n",
    "    \"\"\"\n",
    "    Randomly sample rows from multiple CSV files and combine them into one CSV.\n",
    "    \"\"\"\n",
    "    # Convert to absolute path and resolve any symlinks\n",
    "    input_path = Path(input_folder).resolve()\n",
    "    \n",
    "    print(f\"Looking for CSV files in: {input_path}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not input_path.exists():\n",
    "        raise ValueError(f\"Directory does not exist: {input_path}\")\n",
    "    \n",
    "    # Get list of all CSV files in the input folder\n",
    "    csv_files = list(input_path.glob('*.csv'))\n",
    "    \n",
    "    # Print all files in directory for debugging\n",
    "    print(\"\\nAll files in directory:\")\n",
    "    for file in input_path.iterdir():\n",
    "        print(f\"- {file.name}\")\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {input_path}\")\n",
    "    \n",
    "    print(f\"\\nFound {len(csv_files)} CSV files\")\n",
    "    print(\"\\nCSV files to be processed:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"- {file.name}\")\n",
    "    \n",
    "    # Initialize empty list to store dataframes\n",
    "    combined_data = []\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in tqdm(csv_files, desc=\"Processing files\"):\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            print(f\"\\nProcessing {csv_file.name}\")\n",
    "            print(f\"Original size: {len(df)} rows\")\n",
    "            \n",
    "            # Randomly sample rows\n",
    "            if len(df) >= sample_size:\n",
    "                sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "            else:\n",
    "                print(f\"Warning: {csv_file.name} has fewer than {sample_size} rows. Using all available rows.\")\n",
    "                sampled_df = df\n",
    "            \n",
    "            # Add source file information\n",
    "            sampled_df['source_file'] = csv_file.name\n",
    "            \n",
    "            print(f\"Sampled size: {len(sampled_df)} rows\")\n",
    "            \n",
    "            # Append to list\n",
    "            combined_data.append(sampled_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file.name}: {str(e)}\")\n",
    "    \n",
    "    # Combine all sampled dataframes\n",
    "    if combined_data:\n",
    "        final_df = pd.concat(combined_data, ignore_index=True)\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        output_path = Path(output_file)\n",
    "        os.makedirs(output_path.parent, exist_ok=True)\n",
    "        \n",
    "        # Save the combined dataset\n",
    "        final_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Total rows in combined dataset: {len(final_df)}\")\n",
    "        print(f\"Output saved to: {output_path}\")\n",
    "        \n",
    "        # Print sample distribution\n",
    "        print(\"\\nSamples per source file:\")\n",
    "        print(final_df['source_file'].value_counts())\n",
    "    else:\n",
    "        raise ValueError(\"No data was successfully processed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get the current working directory\n",
    "    current_dir = Path.cwd()\n",
    "    print(f\"Script running from: {current_dir}\")\n",
    "    \n",
    "    # # Try to find the ZDATA directory\n",
    "    # project_dir = current_dir\n",
    "    # while project_dir.name and not (project_dir / \"PENS-Personalized-News-Headline-Generation\").exists():\n",
    "    #     project_dir = project_dir.parent\n",
    "    \n",
    "    # if not project_dir.name:\n",
    "    #     raise ValueError(\"Could not find PENS-Personalized-News-Headline-Generation directory\")\n",
    "    \n",
    "    # Set up paths\n",
    "    input_folder = project_dir / \"your-corresponding-directory\"\n",
    "    output_file = input_folder / \"combined_sample.csv\"\n",
    "    \n",
    "    print(f\"\\nUsing paths:\")\n",
    "    print(f\"Input folder: {input_folder}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    \n",
    "    # Run the function\n",
    "    try:\n",
    "        sample_and_combine_csvs(input_folder, output_file)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e5aaa3-aa27-43bf-85c4-c900d343f4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
