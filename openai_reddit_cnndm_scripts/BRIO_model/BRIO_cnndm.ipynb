{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Yale-LILY/brio-cnndm-uncased\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Yale-LILY/brio-cnndm-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/teamspace/studios/this_studio/NLP_project/openai_cnndm_final.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6291 entries, 0 to 6290\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0.1  6291 non-null   int64 \n",
      " 1   Unnamed: 0    6291 non-null   int64 \n",
      " 2   user_id       6291 non-null   object\n",
      " 3   user_profile  6291 non-null   object\n",
      " 4   doc_id        6291 non-null   object\n",
      " 5   post/article  6291 non-null   object\n",
      " 6   summary_text  6291 non-null   object\n",
      " 7   confidence    6291 non-null   int64 \n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 393.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BartTokenizerFast(name_or_path='Yale-LILY/brio-cnndm-uncased', vocab_size=50265, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       " },\n",
       " BartForConditionalGeneration(\n",
       "   (model): BartModel(\n",
       "     (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "     (encoder): BartEncoder(\n",
       "       (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x BartEncoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): BartDecoder(\n",
       "       (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x BartDecoderLayer(\n",
       "           (self_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartSdpaAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['doc_id', 'post/article']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6291 entries, 0 to 6290\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   doc_id        6291 non-null   object\n",
      " 1   post/article  6291 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 98.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = {}\n",
    "\n",
    "for idx , row in data.iterrows():\n",
    "    doc_id = row['doc_id']\n",
    "    text = row['post/article']\n",
    "\n",
    "    if doc_id not in mapp:\n",
    "        mapp[doc_id] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list(mapp.keys())\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_texts = list(mapp.values())\n",
    "len(unique_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "\n",
    "def translate_batch(texts):\n",
    "    inputs = tokenizer(texts, max_length=1024, truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
    "\n",
    "    summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_texts = unique_texts\n",
    "len(src_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(src_texts), batch_size):\n",
    "    batch_texts = src_texts[i:i + batch_size]\n",
    "    translated_texts = translate_batch(batch_texts)\n",
    "    results.extend(translated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_summaries = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapp = dict(zip(ids, gen_summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['brio_model_summary'] = df['doc_id'].map(mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6291 entries, 0 to 6290\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0.1        6291 non-null   int64 \n",
      " 1   Unnamed: 0          6291 non-null   int64 \n",
      " 2   user_id             6291 non-null   object\n",
      " 3   user_profile        6291 non-null   object\n",
      " 4   doc_id              6291 non-null   object\n",
      " 5   post/article        6291 non-null   object\n",
      " 6   summary_text        6291 non-null   object\n",
      " 7   confidence          6291 non-null   int64 \n",
      " 8   brio_model_summary  6291 non-null   object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 442.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('openai_cnndm_final_brio.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('openai_cnndm_final_brio.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6291 entries, 0 to 6290\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0.2        6291 non-null   int64 \n",
      " 1   Unnamed: 0.1        6291 non-null   int64 \n",
      " 2   Unnamed: 0          6291 non-null   int64 \n",
      " 3   user_id             6291 non-null   object\n",
      " 4   user_profile        6291 non-null   object\n",
      " 5   doc_id              6291 non-null   object\n",
      " 6   post/article        6291 non-null   object\n",
      " 7   summary_text        6291 non-null   object\n",
      " 8   confidence          6291 non-null   int64 \n",
      " 9   brio_model_summary  6291 non-null   object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 491.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wooden deckchair was salvaged from the Atlantic Ocean after the Titanic sank in 1912',\n",
       " 'wooden deckchair was salvaged from the Atlantic Ocean after the Titanic sank in 1912')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[30]['brio_model_summary'], results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
